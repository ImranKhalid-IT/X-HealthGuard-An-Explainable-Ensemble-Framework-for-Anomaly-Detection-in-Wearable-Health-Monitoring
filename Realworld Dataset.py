# -*- coding: utf-8 -*-
"""Untitled31.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18-eqeJ7CVlx1Cir1wELSB0CV03ozNIM7
"""

# Run this cell first to install everything
!pip install wfdb py-ecg-detectors scipy scikit-learn pandas matplotlib seaborn

!pip install biosppy peakutils wfdb scipy scikit-learn pandas matplotlib seaborn

# ===================================================================
# ===== FINAL WORKING SCRIPT: PASTE THIS INTO A SINGLE CELL =====
# ===================================================================

# Step 1: Install all required libraries for the session
!pip install biosppy peakutils wfdb scipy scikit-learn pandas matplotlib seaborn

# Step 2: The full script will run immediately after installation
import numpy as np
import pandas as pd
import wfdb
import os
from sklearn.model_selection import train_test_split
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.svm import OneClassSVM
from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from scipy import signal as sp_signal
from biosppy.signals import ecg

warnings.filterwarnings('ignore', category=FutureWarning)

# --- Configuration ---
WINDOW_SIZE = 720  # 2 seconds (360 Hz * 2)
RECORDS_TO_USE = ['100', '101', '103', '105', '112', '116', '119', '200', '203', '210', '215', '222', '231']

def extract_hrv_features_from_peaks(all_r_peaks, window_start, window_end, fs):
    """
    Calculates HRV features from a list of pre-detected R-peaks that fall within a given window.
    """
    # Find the R-peaks that are inside the current window
    peaks_in_window = all_r_peaks[(all_r_peaks >= window_start) & (all_r_peaks < window_end)]

    if len(peaks_in_window) < 2:
        return None # Not enough peaks to calculate variability

    # Calculate RR-intervals in milliseconds
    rr_intervals = np.diff(peaks_in_window) * (1000.0 / fs)

    if len(rr_intervals) == 0:
        return None

    mean_rr = np.mean(rr_intervals)
    sdnn = np.std(rr_intervals)
    rmssd = np.sqrt(np.mean(np.diff(rr_intervals) ** 2))
    n_peaks = len(peaks_in_window)

    return np.array([n_peaks, mean_rr, sdnn, rmssd])

def process_ecg_data(records: list, window_size: int) -> (np.ndarray, np.ndarray):
    """
    A robust pipeline to load, process, and extract features from ECG data.
    """
    all_features = []
    all_labels = []
    db_dir = 'mitdb_data'

    print(f"Ensuring MIT-BIH database is available locally in '{db_dir}/'...")
    wfdb.dl_database('mitdb', dl_dir=db_dir, records=records, keep_subdirs=False)
    print("Database download/check complete.")

    print(f"\nLoading and processing {len(records)} records...")

    for record_name in records:
        print(f"  - Processing record: {record_name}")
        try:
            record_path = os.path.join(db_dir, record_name)
            record = wfdb.rdrecord(record_path)
            annotation = wfdb.rdann(record_path, 'atr')

            fs = record.fs
            signal = record.p_signal[:, 0]

            # --- NEW WORKFLOW ---
            # 1. Run biosppy's detector ONCE on the ENTIRE signal
            ecg_data = ecg.ecg(signal=signal, sampling_rate=fs, show=False)
            all_r_peaks = ecg_data['rpeaks']

            ann_symbols = annotation.symbol
            ann_locations = annotation.sample

            # 2. Now, slide the window and extract features from the pre-computed peaks
            for i in range(0, len(signal) - window_size, window_size):
                window_start = i
                window_end = i + window_size

                window_annotations = [sym for loc, sym in zip(ann_locations, ann_symbols) if window_start <= loc < window_end]
                is_anomaly = any(symbol != 'N' for symbol in window_annotations)

                features = extract_hrv_features_from_peaks(all_r_peaks, window_start, window_end, fs)

                if features is None or np.any(np.isnan(features)) or np.any(np.isinf(features)):
                    continue

                all_features.append(features)
                all_labels.append(1 if is_anomaly else 0)

        except Exception as e:
            print(f"    Could not process record {record_name}. Reason: {e}")

    print("Data loading complete.")
    return np.array(all_features), np.array(all_labels)


if __name__ == "__main__":
    X, y = process_ecg_data(RECORDS_TO_USE, WINDOW_SIZE)

    if len(X) > 0:
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42, stratify=y)

        X_train_normal = X_train[y_train == 0]
        print(f"\nTotal data points processed: {len(X)}")
        print(f"Training on {len(X_train_normal)} 'Normal' samples.")
        print(f"Testing on {len(X_test)} mixed samples ({np.sum(y_test)} anomalies).")

        models = {
            'IsolationForest': IsolationForest(contamination='auto', random_state=42),
            'LOF': LocalOutlierFactor(novelty=True, contamination='auto'),
            'OneClassSVM': OneClassSVM(nu=0.01, kernel="rbf", gamma='auto')
        }

        print("\n--- Training Models ---")
        for name, model in models.items():
            model.fit(X_train_normal)

        print("\n--- Evaluating Models ---")
        results = {}

        scores_if_train = models['IsolationForest'].decision_function(X_train_normal)
        scores_lof_train = models['LOF'].decision_function(X_train_normal)
        scores_ocsvm_train = models['OneClassSVM'].decision_function(X_train_normal)
        ensemble_scores_train_normal = - (scores_if_train + scores_lof_train + scores_ocsvm_train) / 3.0
        anomaly_threshold = np.percentile(ensemble_scores_train_normal, 95)
        print(f"Anomaly threshold set to: {anomaly_threshold:.4f} (based on 95th percentile of normal data)")

        scores_if_test = models['IsolationForest'].decision_function(X_test)
        scores_lof_test = models['LOF'].decision_function(X_test)
        scores_ocsvm_test = models['OneClassSVM'].decision_function(X_test)
        ensemble_scores_test = - (scores_if_test + scores_lof_test + scores_ocsvm_test) / 3.0

        y_pred_ensemble = (ensemble_scores_test > anomaly_threshold).astype(int)

        prec, rec, f1, _ = precision_recall_fscore_support(y_test, y_pred_ensemble, average='binary', zero_division=0)
        auc = roc_auc_score(y_test, ensemble_scores_test)
        results['Ensemble'] = {'Precision': prec, 'Recall': rec, 'F1-Score': f1, 'AUC': auc}

        print("\n--- Final Performance on MIT-BIH Dataset ---")
        df_results = pd.DataFrame(results).T.round(3)
        print(df_results.to_string())

        cm = confusion_matrix(y_test, y_pred_ensemble)
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=['Predicted Normal', 'Predicted Anomaly'],
                    yticklabels=['True Normal', 'True Anomaly'])
        plt.title('Final Confusion Matrix')
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')
        plt.savefig('confusion_matrix_final.png')
        plt.show()
        print("\nFinal confusion matrix saved as 'confusion_matrix_final.png'")
    else:
        print("\nNo data was loaded. Please check the record names and network connection.")

!pip install tensorflow imbalanced-learn

# ===================================================================
# ===== FINAL SCRIPT FOR >90% ACCURACY: PASTE INTO A SINGLE CELL =====
# ===================================================================

# Step 1: Install all required libraries for the session
!pip install biosppy peakutils wfdb scipy scikit-learn pandas matplotlib seaborn tensorflow imbalanced-learn

# Step 2: The full script will run immediately after installation
import numpy as np
import pandas as pd
import wfdb
import os
import warnings
import matplotlib.pyplot as plt
import seaborn as sns

# Import for signal processing
from biosppy.signals import ecg

# Imports for ML and data preparation
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from imblearn.over_sampling import SMOTE

# Imports for Deep Learning (1D-CNN)
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.utils import to_categorical

warnings.filterwarnings('ignore')

# --- Configuration ---
# We will use the full database for better results
RECORDS_TO_USE = wfdb.get_record_list('mitdb')
# We will exclude records that are not suitable for beat classification
RECORDS_TO_EXCLUDE = ['102', '104', '107', '217'] # These have paced beats or other issues
RECORDS_TO_USE = [r for r in RECORDS_TO_USE if r not in RECORDS_TO_EXCLUDE]

# Each heartbeat will be a window of 256 samples centered on the R-peak
WINDOW_SIZE = 256

def prepare_beat_dataset(records: list, window_size: int) -> (np.ndarray, np.ndarray):
    """
    Creates a supervised dataset of individual heartbeats.
    Each sample is a window centered around an R-peak.
    """
    all_beats = []
    all_labels = []
    db_dir = 'mitdb_data'

    # Define which beat annotations are 'Normal' and which are 'Anomaly'
    # This is a standard mapping for MIT-BIH
    normal_symbols = ['N', 'L', 'R', 'e', 'j']

    print(f"Ensuring MIT-BIH database is available locally in '{db_dir}/'...")
    wfdb.dl_database('mitdb', dl_dir=db_dir, records=records, keep_subdirs=False)
    print("Database download/check complete.")

    print(f"\nProcessing {len(records)} records to extract individual heartbeats...")
    for record_name in records:
        try:
            record_path = os.path.join(db_dir, record_name)
            record = wfdb.rdrecord(record_path)
            annotation = wfdb.rdann(record_path, 'atr')

            fs = record.fs
            signal = record.p_signal[:, 0]

            # Use biosppy to find all R-peaks in the entire signal
            ecg_data = ecg.ecg(signal=signal, sampling_rate=fs, show=False)
            r_peaks = ecg_data['rpeaks']

            # Match each R-peak to its annotation label
            for r_peak in r_peaks:
                # Find the closest annotation to the R-peak
                closest_ann_idx = np.argmin(np.abs(annotation.sample - r_peak))
                symbol = annotation.symbol[closest_ann_idx]

                # Create a window around the R-peak
                start, end = r_peak - window_size // 2, r_peak + window_size // 2
                if start < 0 or end > len(signal):
                    continue

                beat_window = signal[start:end]

                # Append the beat and its label (0 for Normal, 1 for Anomaly)
                all_beats.append(beat_window)
                all_labels.append(0 if symbol in normal_symbols else 1)
        except Exception as e:
            print(f"    Could not process record {record_name}. Reason: {e}")

    print("Beat extraction complete.")
    return np.array(all_beats), np.array(all_labels)

def create_1d_cnn_model(input_shape):
    """Creates a 1D-CNN model architecture suitable for ECG beat classification."""
    model = Sequential([
        Conv1D(filters=64, kernel_size=6, activation='relu', input_shape=input_shape),
        BatchNormalization(),
        MaxPooling1D(pool_size=3),

        Conv1D(filters=128, kernel_size=6, activation='relu'),
        BatchNormalization(),
        MaxPooling1D(pool_size=3),

        Flatten(),
        Dense(128, activation='relu'),
        Dropout(0.5),
        Dense(2, activation='softmax') # 2 output neurons for 2 classes (Normal, Anomaly)
    ])

    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

if __name__ == "__main__":
    # 1. Prepare the beat-by-beat dataset
    X, y = prepare_beat_dataset(RECORDS_TO_USE, WINDOW_SIZE)

    # Reshape X for the CNN: (n_samples, n_timesteps, n_features)
    X = X.reshape(X.shape[0], X.shape[1], 1)
    # Convert y to categorical format for the CNN
    y_cat = to_categorical(y)

    # 2. Split into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y_cat, test_size=0.2, random_state=42, stratify=y)

    print(f"\nDataset shape: {X.shape}")
    print(f"Training set shape: {X_train.shape}")
    print(f"Testing set shape: {X_test.shape}")

    # 3. Balance the TRAINING data using SMOTE
    print("\nBalancing the training data with SMOTE...")
    # SMOTE works with 2D data, so we need to reshape
    n_samples, n_timesteps, n_features = X_train.shape
    X_train_reshaped = X_train.reshape(n_samples, n_timesteps * n_features)
    y_train_labels = np.argmax(y_train, axis=1) # SMOTE needs 1D labels

    smote = SMOTE(random_state=42)
    X_train_smote, y_train_smote = smote.fit_resample(X_train_reshaped, y_train_labels)

    # Reshape back to 3D for the CNN
    X_train_smote = X_train_smote.reshape(-1, n_timesteps, n_features)
    y_train_smote_cat = to_categorical(y_train_smote)
    print(f"Training set shape after SMOTE: {X_train_smote.shape}")

    # 4. Create and train the 1D-CNN model
    input_shape = (WINDOW_SIZE, 1)
    model = create_1d_cnn_model(input_shape)
    model.summary()

    print("\n--- Training the 1D-CNN Model ---")
    history = model.fit(X_train_smote, y_train_smote_cat, epochs=15, batch_size=128, validation_split=0.1)

    # 5. Evaluate the model on the UNSEEN test set
    print("\n--- Evaluating Model Performance ---")
    y_pred = model.predict(X_test)
    y_pred_labels = np.argmax(y_pred, axis=1)
    y_test_labels = np.argmax(y_test, axis=1)

    # Calculate accuracy
    accuracy = accuracy_score(y_test_labels, y_pred_labels)
    print(f"Final Test Accuracy: {accuracy * 100:.2f}%")

    # Print detailed classification report
    print("\nClassification Report:")
    print(classification_report(y_test_labels, y_pred_labels, target_names=['Normal (0)', 'Anomaly (1)']))

    # Display confusion matrix
    cm = confusion_matrix(y_test_labels, y_pred_labels)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['Normal', 'Anomaly'],
                yticklabels=['Normal', 'Anomaly'])
    plt.title('Confusion Matrix for 1D-CNN Model')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.savefig('confusion_matrix_cnn_final.png')
    plt.show()

# =================================================================================
# FINAL SCRIPT TO GENERATE ALL TABLES AND FIGURES FOR THE X-HEALTHGUARD PAPER
# =================================================================================
# This script will run three separate experiments and save all outputs.
# 1. The original experiment on synthetic data.
# 2. The real-world validation of X-HealthGuard with HRV features.
# 3. The state-of-the-art 1D-CNN benchmark model.
# =================================================================================

# --- Step 1: Install all required libraries for the session ---
!pip install biosppy peakutils wfdb scipy scikit-learn pandas matplotlib seaborn tensorflow imbalanced-learn

# --- Step 2: Import all necessary modules ---
import numpy as np
import pandas as pd
import wfdb
import os
import time
import random
import warnings
import matplotlib.pyplot as plt
import seaborn as sns

# Signal Processing
from biosppy.signals import ecg

# Machine Learning
from sklearn.ensemble import IsolationForest, RandomForestClassifier
from sklearn.neighbors import LocalOutlierFactor
from sklearn.svm import OneClassSVM
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    precision_recall_fscore_support, roc_auc_score, confusion_matrix,
    roc_curve, auc, classification_report, accuracy_score
)
from imblearn.over_sampling import SMOTE

# Deep Learning
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.utils import to_categorical

warnings.filterwarnings('ignore')
sns.set_theme(style="whitegrid")

# =================================================================================
# PART 1: SYNTHETIC DATA EXPERIMENT (Original Paper Results)
# =================================================================================

def run_synthetic_experiment():
    print("\n" + "="*80)
    print("RUNNING PART 1: SYNTHETIC DATA EXPERIMENT")
    print("="*80 + "\n")

    # --- Data Generation ---
    N_SAMPLES_TRAIN = 2000
    N_SAMPLES_TEST = 200
    WINDOW_SIZE = 30

    def extract_simple_features(window):
        return np.array([
            np.mean(window), np.std(window), np.max(window),
            np.min(window), np.max(window) - np.min(window)
        ])

    def generate_synthetic_data(n_samples, is_test=False):
        feature_list, labels = [], []
        anomaly_types = ['Spike', 'Volatility']
        anomaly_indices = random.sample(range(n_samples), int(n_samples * 0.2)) if is_test else []
        for i in range(n_samples):
            if i in anomaly_indices:
                anomaly_type = random.choice(anomaly_types)
                labels.append(anomaly_type)
                if anomaly_type == 'Spike':
                    window = 50 + (np.random.rand(WINDOW_SIZE) - 0.5) * 20
                    window[random.randint(10, 20)] = 180.0
                else: # Volatility
                    window = 50 + (np.random.rand(WINDOW_SIZE) - 0.5) * 80
            else:
                labels.append('Normal')
                window = 50 + (np.random.rand(WINDOW_SIZE) - 0.5) * 30
            feature_list.append(extract_simple_features(window))
        return np.array(feature_list), labels

    # --- Train Models ---
    train_features, _ = generate_synthetic_data(N_SAMPLES_TRAIN)
    test_features, test_labels = generate_synthetic_data(N_SAMPLES_TEST, is_test=True)

    models = {
        'IsolationForest': IsolationForest(contamination='auto', random_state=42),
        'LOF': LocalOutlierFactor(novelty=True, contamination='auto'),
        'OneClassSVM': OneClassSVM(nu=0.01, kernel="rbf", gamma='auto')
    }
    thresholds = {}
    for name, model in models.items():
        model.fit(train_features)
        scores = model.decision_function(train_features)
        thresholds[name] = np.percentile(scores, 5)

    # --- Evaluate Models ---
    true_labels_numeric = [0 if l == 'Normal' else 1 for l in test_labels]
    results = {}
    for name, model in models.items():
        scores = model.decision_function(test_features)
        preds = (scores < thresholds[name]).astype(int)
        prec, rec, f1, _ = precision_recall_fscore_support(true_labels_numeric, preds, average='binary')
        roc_auc = roc_auc_score(true_labels_numeric, -scores)
        results[name] = {'Precision': prec, 'Recall': rec, 'F1-Score': f1, 'AUC': roc_auc}

    # Ensemble evaluation
    ensemble_scores = np.mean([model.decision_function(test_features) for model in models.values()], axis=0)
    ensemble_threshold = np.mean(list(thresholds.values()))
    ensemble_preds = (ensemble_scores < ensemble_threshold).astype(int)
    prec, rec, f1, _ = precision_recall_fscore_support(true_labels_numeric, ensemble_preds, average='binary')
    roc_auc = roc_auc_score(true_labels_numeric, -ensemble_scores)
    results['Ensemble'] = {'Precision': prec, 'Recall': rec, 'F1-Score': f1, 'AUC': roc_auc}

    # --- Generate Tables & Figures ---

    # Table 5: Overall Performance
    print("--- Generating Table 5: Overall Performance (Synthetic) ---")
    df_perf = pd.DataFrame(results).T.round(3)[['Precision', 'Recall', 'F1-Score', 'AUC']]
    df_perf.to_csv('table_5_synthetic_performance.txt', sep='\t')
    print(df_perf)

    # Figure 3: Confusion Matrix
    print("\n--- Saving Figure 3: Confusion Matrix (Synthetic) ---")
    cm = confusion_matrix(true_labels_numeric, ensemble_preds)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Anomaly'], yticklabels=['Normal', 'Anomaly'])
    plt.title('Confusion Matrix for Ensemble Model (Synthetic Data)')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.savefig('figure_3_synthetic_confusion_matrix.png')
    plt.close()

    # Table 11: Update (only the X-HealthGuard row is generated here)
    print("\n--- Generating Data for Table 11: SoTA Comparison ---")
    df_sota_xhealthguard = pd.DataFrame({
        'Reference': ['X-HealthGuard (This Work)'],
        'Method / Model': ['Feature Engineering + Ensemble'],
        'Primary Task': ['Health Anomaly Detection'],
        'Performance': [f"{results['Ensemble']['F1-Score']:.3f} (F1-Score on Synthetic Data)"],
        'Explainability (XAI)': ['Yes (Counterfactual)'],
        'Quantitative XAI Validation': ['Yes (ECS)'],
        'Ensemble Robustness': ['Yes']
    })
    df_sota_xhealthguard.to_csv('table_11_xhealthguard_row.txt', sep='\t', index=False)
    print("Data for X-HealthGuard row:")
    print(df_sota_xhealthguard.to_string(index=False))

# =================================================================================
# PART 2: REAL-WORLD HRV EXPERIMENT (X-HealthGuard on MIT-BIH)
# =================================================================================

def run_real_world_hrv_experiment():
    print("\n" + "="*80)
    print("RUNNING PART 2: REAL-WORLD HRV EXPERIMENT")
    print("="*80 + "\n")

    # --- Config ---
    WINDOW_SIZE = 720
    RECORDS_TO_USE = ['100', '101', '103', '105', '112', '116', '119', '200', '203', '210', '215', '222', '231']

    # --- Feature Extraction ---
    def extract_hrv_features_from_peaks(all_r_peaks, window_start, window_end, fs):
        peaks_in_window = all_r_peaks[(all_r_peaks >= window_start) & (all_r_peaks < window_end)]
        if len(peaks_in_window) < 2: return None
        rr_intervals = np.diff(peaks_in_window) * (1000.0 / fs)
        if len(rr_intervals) == 0: return None
        return np.array([
            len(peaks_in_window), np.mean(rr_intervals),
            np.std(rr_intervals), np.sqrt(np.mean(np.diff(rr_intervals) ** 2))
        ])

    def process_ecg_data(records, window_size):
        all_features, all_labels = [], []
        db_dir = 'mitdb_data'
        wfdb.dl_database('mitdb', dl_dir=db_dir, records=records, keep_subdirs=False)
        for record_name in records:
            record_path = os.path.join(db_dir, record_name)
            record = wfdb.rdrecord(record_path)
            annotation = wfdb.rdann(record_path, 'atr')
            fs = record.fs
            signal = record.p_signal[:, 0]
            ecg_data = ecg.ecg(signal=signal, sampling_rate=fs, show=False)
            all_r_peaks = ecg_data['rpeaks']
            for i in range(0, len(signal) - window_size, window_size):
                window_annotations = [sym for loc, sym in zip(annotation.sample, annotation.symbol) if i <= loc < i + window_size]
                is_anomaly = any(symbol != 'N' for symbol in window_annotations)
                features = extract_hrv_features_from_peaks(all_r_peaks, i, i + window_size, fs)
                if features is not None and not np.any(np.isnan(features)):
                    all_features.append(features)
                    all_labels.append(1 if is_anomaly else 0)
        return np.array(all_features), np.array(all_labels)

    # --- Train and Evaluate ---
    X, y = process_ecg_data(RECORDS_TO_USE, WINDOW_SIZE)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42, stratify=y)
    X_train_normal = X_train[y_train == 0]

    models = {
        'IsolationForest': IsolationForest(contamination='auto', random_state=42),
        'LOF': LocalOutlierFactor(novelty=True, contamination='auto'),
        'OneClassSVM': OneClassSVM(nu=0.01, kernel="rbf", gamma='auto')
    }
    for name, model in models.items():
        model.fit(X_train_normal)

    ensemble_scores_train_normal = -np.mean([m.decision_function(X_train_normal) for m in models.values()], axis=0)
    anomaly_threshold = np.percentile(ensemble_scores_train_normal, 95)

    ensemble_scores_test = -np.mean([m.decision_function(X_test) for m in models.values()], axis=0)
    y_pred_ensemble = (ensemble_scores_test > anomaly_threshold).astype(int)

    # --- Generate Table & Figure ---
    print("--- Generating NEW Table: X-HealthGuard Performance on MIT-BIH ---")
    prec, rec, f1, _ = precision_recall_fscore_support(y_test, y_pred_ensemble, average='binary', zero_division=0)
    auc_score = roc_auc_score(y_test, ensemble_scores_test)
    df_hrv_results = pd.DataFrame({
        'Metric': ['Precision', 'Recall', 'F1-Score', 'AUC'],
        'Score': [prec, rec, f1, auc_score]
    }).round(3)
    df_hrv_results.to_csv('table_X_hrv_performance.txt', sep='\t', index=False)
    print(df_hrv_results)

    print("\n--- Saving NEW Figure: X-HealthGuard Confusion Matrix on MIT-BIH ---")
    cm_hrv = confusion_matrix(y_test, y_pred_ensemble)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm_hrv, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Anomaly'], yticklabels=['Normal', 'Anomaly'])
    plt.title('Confusion Matrix for X-HealthGuard (HRV Features) on MIT-BIH Data')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.savefig('figure_Y_hrv_confusion_matrix.png')
    plt.close()

# =================================================================================
# PART 3: 1D-CNN DEEP LEARNING EXPERIMENT (SOTA Benchmark)
# =================================================================================

def run_deep_learning_cnn_experiment():
    print("\n" + "="*80)
    print("RUNNING PART 3: 1D-CNN DEEP LEARNING EXPERIMENT")
    print("="*80 + "\n")

    # --- Config ---
    WINDOW_SIZE_CNN = 256
    RECORDS_TO_USE_CNN = [r for r in wfdb.get_record_list('mitdb') if r not in ['102', '104', '107', '217']]

    # --- Data Preparation ---
    def prepare_beat_dataset(records, window_size):
        all_beats, all_labels = [], []
        db_dir = 'mitdb_data'
        normal_symbols = ['N', 'L', 'R', 'e', 'j']
        wfdb.dl_database('mitdb', dl_dir=db_dir, records=records, keep_subdirs=False)
        for record_name in records:
            record_path = os.path.join(db_dir, record_name)
            record = wfdb.rdrecord(record_path)
            annotation = wfdb.rdann(record_path, 'atr')
            signal = record.p_signal[:, 0]
            r_peaks = ecg.ecg(signal=signal, sampling_rate=record.fs, show=False)['rpeaks']
            for r_peak in r_peaks:
                closest_ann_idx = np.argmin(np.abs(annotation.sample - r_peak))
                symbol = annotation.symbol[closest_ann_idx]
                start, end = r_peak - window_size // 2, r_peak + window_size // 2
                if start >= 0 and end <= len(signal):
                    all_beats.append(signal[start:end])
                    all_labels.append(0 if symbol in normal_symbols else 1)
        return np.array(all_beats), np.array(all_labels)

    # --- Model Definition ---
    def create_1d_cnn_model(input_shape):
        model = Sequential([
            Conv1D(filters=64, kernel_size=6, activation='relu', input_shape=input_shape),
            BatchNormalization(), MaxPooling1D(pool_size=3),
            Conv1D(filters=128, kernel_size=6, activation='relu'),
            BatchNormalization(), MaxPooling1D(pool_size=3),
            Flatten(),
            Dense(128, activation='relu'), Dropout(0.5),
            Dense(2, activation='softmax')
        ])
        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
        return model

    # --- Run Pipeline ---
    X, y = prepare_beat_dataset(RECORDS_TO_USE_CNN, WINDOW_SIZE_CNN)
    X = X.reshape(X.shape[0], X.shape[1], 1)
    y_cat = to_categorical(y)
    X_train, X_test, y_train, y_test = train_test_split(X, y_cat, test_size=0.2, random_state=42, stratify=y)

    n_samples, n_timesteps, n_features = X_train.shape
    X_train_reshaped = X_train.reshape(n_samples, n_timesteps * n_features)
    y_train_labels = np.argmax(y_train, axis=1)
    smote = SMOTE(random_state=42)
    X_train_smote, y_train_smote = smote.fit_resample(X_train_reshaped, y_train_labels)
    X_train_smote = X_train_smote.reshape(-1, n_timesteps, n_features)
    y_train_smote_cat = to_categorical(y_train_smote)

    model = create_1d_cnn_model((WINDOW_SIZE_CNN, 1))
    history = model.fit(X_train_smote, y_train_smote_cat, epochs=15, batch_size=128, validation_split=0.1, verbose=0)

    y_pred = model.predict(X_test)
    y_pred_labels = np.argmax(y_pred, axis=1)
    y_test_labels = np.argmax(y_test, axis=1)

    # --- Generate Tables & Figures ---
    print("--- Generating NEW Table: 1D-CNN Performance (Classification Report) ---")
    report = classification_report(y_test_labels, y_pred_labels, target_names=['Normal (0)', 'Anomaly (1)'], output_dict=True)
    df_report = pd.DataFrame(report).transpose().round(3)
    df_report.to_csv('table_Z_cnn_classification_report.txt', sep='\t')
    print(df_report)

    print("\n--- Saving NEW Figure: 1D-CNN Confusion Matrix on MIT-BIH ---")
    cm_cnn = confusion_matrix(y_test_labels, y_pred_labels)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm_cnn, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Anomaly'], yticklabels=['Normal', 'Anomaly'])
    plt.title('Confusion Matrix for 1D-CNN Model on MIT-BIH Data')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.savefig('figure_Z_cnn_confusion_matrix.png')
    plt.close()

    print("\n--- Saving NEW Figure: 1D-CNN Training History ---")
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title('CNN Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('CNN Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.tight_layout()
    plt.savefig('figure_W_cnn_training_history.png')
    plt.close()

# =================================================================================
# MAIN EXECUTION BLOCK
# =================================================================================

if __name__ == "__main__":
    run_synthetic_experiment()
    run_real_world_hrv_experiment()
    run_deep_learning_cnn_experiment()

    print("\n" + "="*80)
    print("ALL EXPERIMENTS COMPLETE.")
    print("Please check the file browser for the following generated files:")
    print("\n--- Synthetic Data Files ---")
    print("- table_5_synthetic_performance.txt")
    print("- figure_3_synthetic_confusion_matrix.png")
    print("- table_11_xhealthguard_row.txt")
    print("\n--- Real-World HRV Files ---")
    print("- table_X_hrv_performance.txt")
    print("- figure_Y_hrv_confusion_matrix.png")
    print("\n--- Deep Learning CNN Files ---")
    print("- table_Z_cnn_classification_report.txt")
    print("- figure_Z_cnn_confusion_matrix.png")
    print("- figure_W_cnn_training_history.png")
    print("="*80)